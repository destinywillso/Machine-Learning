{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd770d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99cab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path\n",
    "image_path = \"images-trainval\"\n",
    "train_path = r\"jsonl\\train.jsonl\"\n",
    "info_path = \"jsonl\\info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data process\n",
    "with open(info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    info = json.load(f)\n",
    "print(\"Loaded info.json. Examples:\", type(info))\n",
    "\n",
    "annotations = {}\n",
    "data = []\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Reading_train.jsonl\"):\n",
    "        item = json.loads(line)\n",
    "        image_id = item.get(\"image_id\") \n",
    "        image_file = os.path.join(image_path, f\"{image_id}.jpg\")\n",
    "        if os.path.exists(image_file):\n",
    "            data.append(item)\n",
    "        else:\n",
    "            print(f\"There is no image {image_id}.jpg.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = defaultdict(lambda: {\n",
    "    \"annotations\": [],\n",
    "    \"ignore\": [],\n",
    "    \"file_name\": None,\n",
    "    \"height\": None,\n",
    "    \"width\": None\n",
    "})\n",
    "\n",
    "for item in data:\n",
    "    image_id = item.get(\"image_id\")\n",
    "    \n",
    "    for annotation in item.get(\"annotations\", []):\n",
    "        annotations[image_id][\"annotations\"].append(annotation)\n",
    "\n",
    "    for i in item.get(\"ignore\", []):\n",
    "        annotations[image_id][\"ignore\"].append(i)\n",
    "\n",
    "    annotations[image_id][\"file_name\"] = item.get(\"file_name\")\n",
    "    annotations[image_id][\"height\"] = item.get(\"height\")\n",
    "    annotations[image_id][\"width\"] = item.get(\"width\")\n",
    "\n",
    "print(\"Total images found in the file:\", len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask\n",
    "\n",
    "def Mask_Function(image_id):\n",
    "    image_file_mask = os.path.join(image_path, f\"{image_id}.jpg\")\n",
    "    image = cv2.imread(image_file_mask)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for annotation in annotations[image_id][\"annotations\"]:\n",
    "        for bbox_info in annotation:\n",
    "            x, y, w, h = map(int, bbox_info[\"adjusted_bbox\"])\n",
    "            x1, y1 = max(0, x), max(0, y)\n",
    "            x2, y2 = min(width, x+w), min(height, y+h)\n",
    "            mask[y1:y2, x1:x2] = 1\n",
    "\n",
    "    return image, mask.astype(np.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa01893",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(annotations.keys())\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "selected_size = 500\n",
    "image_ids = image_ids[:selected_size]\n",
    "\n",
    "train_split = image_ids[:int(0.7*len(image_ids))]\n",
    "value_split = image_ids[int(0.7*len(image_ids)):int(0.85*len(image_ids))]\n",
    "test_split  = image_ids[int(0.85*len(image_ids)):]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "image, mask = Mask_Function(\"0000189\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Mask (Bounding Box Regions)\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.imshow(mask, alpha=0.5, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset1(Dataset):\n",
    "    def __init__(self, image_ids, img_dir, annotations):\n",
    "        self.image_ids = image_ids\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image, mask = Mask_Function(image_id)\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2(Dataset):\n",
    "    def __init__(self, image_ids, img_dir, annotations, patch_size=128, stride=64, threshold=0.01, transform=None):\n",
    "        self.patches = []\n",
    "        self.patch_masks = []\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            image, mask = Mask_Function(image_id)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            for y in range(0, h - patch_size + 1, stride):\n",
    "                for x in range(0, w - patch_size + 1, stride):\n",
    "                    patch = image[y:y+patch_size, x:x+patch_size]\n",
    "                    patch_mask = mask[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "                    if patch_mask.sum() / (patch_size * patch_size) > threshold:\n",
    "                        self.patches.append(patch)\n",
    "                        self.patch_masks.append(patch_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.patches[idx]\n",
    "        patch_mask = self.patch_masks[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=patch, mask=patch_mask)\n",
    "            patch = augmented['image']\n",
    "            patch_mask = augmented['mask']\n",
    "\n",
    "        patch = torch.tensor(patch, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        patch_mask = torch.tensor(patch_mask, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return patch, patch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 2, stride=2), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "\n",
    "        d3 = self.upconv3(b)\n",
    "        if d3.shape != e3.shape:\n",
    "            d3 = F.interpolate(d3, size=e3.shape[2:])\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        if d2.shape != e2.shape:\n",
    "            d2 = F.interpolate(d2, size=e2.shape[2:])\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        if d1.shape != e1.shape:\n",
    "            d1 = F.interpolate(d1, size=e1.shape[2:])\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.conv_final(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detection_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            preds = (preds > threshold).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "            all_targets.extend(masks.cpu().numpy().reshape(-1))\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "    intersection = np.logical_and(all_preds==1, all_targets==1).sum()\n",
    "    union = np.logical_or(all_preds==1, all_targets==1).sum()\n",
    "    iou = intersection / (union + 1e-6)\n",
    "\n",
    "    dice = (2 * intersection) / (all_preds.sum() + all_targets.sum() + 1e-6)\n",
    "\n",
    "    return accuracy,precision,recall,f1,dice,iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset2(train_split, image_path, annotations)\n",
    "value_dataset = Dataset2(value_split, image_path, annotations)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "value_loader = DataLoader(value_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_cnn = CNN_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b003fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred, target, eps=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))\n",
    "    dice = (2. * intersection + eps) / (union + eps)\n",
    "    return dice.mean().item()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_flat = pred.view(pred.size(0), -1)   \n",
    "    target_flat = target.view(target.size(0), -1)\n",
    "\n",
    "    intersection = (pred_flat * target_flat).sum(1)  \n",
    "    union = pred_flat.sum(1) + target_flat.sum(1)  \n",
    "    loss = 1 - (2 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return loss.mean()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac48beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = torch.tensor([15.0]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "num_epochs = 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model_cnn.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn(images)\n",
    "\n",
    "        loss = criterion(outputs, masks) + 2*dice_loss(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model_cnn.eval()\n",
    "    value_dice = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for value_images, value_masks in value_loader:\n",
    "            value_images, value_masks = value_images.to(device), value_masks.to(device)\n",
    "            preds = model_cnn(value_images)\n",
    "\n",
    "            value_dice += dice(preds, value_masks)\n",
    "            count += 1\n",
    "\n",
    "    average_dice = value_dice / count\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dice: {average_dice:.4f}\")\n",
    "\n",
    "torch.save(model_cnn.state_dict(), \"model_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e319fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_model_cnn,precision_model_cnn,recall_model_cnn,f1_model_cnn,dice_model_cnn,iou_model_cnn = evaluate_detection_model(model_cnn, value_loader, device)\n",
    "print(f\"Accuracy: {accuracy_model_cnn*100:.2f}%\")\n",
    "print(f\"Precision: {precision_model_cnn*100:.2f}%\")\n",
    "print(f\"Recall: {recall_model_cnn*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_model_cnn*100:.2f}%\")\n",
    "print(f\"Dice Score: {dice_model_cnn*100:.2f}%\")\n",
    "print(f\"IoU: {iou_model_cnn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet = UNet().to(device)\n",
    "optimizer_unet = torch.optim.Adam(model_unet.parameters(), lr=0.0001)\n",
    "pos_weight_unet = torch.tensor([20]).to(device)\n",
    "criterion_unet = nn.BCEWithLogitsLoss(pos_weight=pos_weight_unet)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_unet.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device) \n",
    "\n",
    "        optimizer_unet.zero_grad()\n",
    "        outputs = model_unet(images)\n",
    "\n",
    "        loss = criterion_unet(outputs, masks) + 2*dice_loss(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer_unet.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model_unet.eval()\n",
    "    value_dice = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for value_images, value_masks in value_loader:\n",
    "            value_images, value_masks = value_images.to(device), value_masks.to(device)  \n",
    "            preds = model_unet(value_images)\n",
    "\n",
    "            value_dice += dice(preds, value_masks)  \n",
    "            count += 1\n",
    "\n",
    "    average_dice = value_dice / count\n",
    "\n",
    "    print(f\"UNet Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dice: {average_dice:.4f}\")\n",
    "\n",
    "torch.save(model_unet.state_dict(), \"model_unet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_model_unet,precision_model_unet,recall_model_unet,f1_model_unet,dice_model_unet,iou_model_unet= evaluate_detection_model(model_unet, value_loader, device)\n",
    "print(f\"Accuracy: {accuracy_model_unet*100:.2f}%\")\n",
    "print(f\"Precision: {precision_model_unet*100:.2f}%\")\n",
    "print(f\"Recall: {recall_model_unet*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_model_unet*100:.2f}%\")\n",
    "print(f\"Dice Score: {dice_model_unet*100:.2f}%\")\n",
    "print(f\"IoU: {iou_model_unet*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa90361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chars = []\n",
    "for image_id, information in annotations.items():\n",
    "    for item in information[\"annotations\"]:  \n",
    "        for annotation in item:\n",
    "            if annotation.get(\"is_chinese\"):\n",
    "                chars.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"char\": annotation[\"text\"],\n",
    "                    \"bbox\": annotation[\"adjusted_bbox\"],\n",
    "                    \"attributes\": annotation[\"attributes\"]\n",
    "                })\n",
    "\n",
    "print(f\"Number of the chars:{len(chars)}\")\n",
    "print(chars[:1])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "count = 0\n",
    "os.makedirs(\"chars\", exist_ok=True)\n",
    "\n",
    "for entry in tqdm(chars):\n",
    "    image_id = entry[\"image_id\"]\n",
    "    char_label = entry[\"char\"]\n",
    "    bbox = entry[\"bbox\"]\n",
    "\n",
    "    image_path = os.path.join(\"images-trainval\", f\"{image_id}.jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"{image_path} pass\")\n",
    "        continue\n",
    "\n",
    "    x, y, w, h = map(int, bbox)\n",
    "    x, y = max(0, x), max(0, y)\n",
    "    w, h = max(1, w), max(1, h)\n",
    "    char_img = image[y:y+h, x:x+w]\n",
    "\n",
    "    save_dir = os.path.join(\"chars\", char_label)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    img_name = f\"{image_id}_{count}.jpg\"\n",
    "    save_path = os.path.join(save_dir, img_name)\n",
    "\n",
    "    _, encoded_image = cv2.imencode(\".jpg\", char_img)\n",
    "    encoded_image.tofile(save_path)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print(f\"Tototal images: {count}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6374d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "records = []\n",
    "\n",
    "for char_label in os.listdir(\"chars\"):\n",
    "    folder = os.path.join(\"chars\", char_label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    for img_name in os.listdir(folder):\n",
    "        records.append({\n",
    "            \"image_path\": os.path.join(folder, img_name),\n",
    "            \"label\": char_label\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"chars_labels.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Total chars and labels:{len(df)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86226f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(\"dataset\", split), exist_ok=True)\n",
    "\n",
    "def copy_image(src, dst):\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "for char_label in tqdm(os.listdir(\"chars\"), desc=\"Processing labels\"):\n",
    "    char_folder = os.path.join(\"chars\", char_label)\n",
    "    images = os.listdir(char_folder)\n",
    "    random.shuffle(images)\n",
    "    total = len(images)\n",
    "    train_end = int(split_ratio[0]*total)\n",
    "    val_end = train_end + int(split_ratio[1]*total)\n",
    "\n",
    "    subsets = {\n",
    "        \"train\": images[:train_end],\n",
    "        \"val\": images[train_end:val_end],\n",
    "        \"test\": images[val_end:]\n",
    "    }\n",
    "\n",
    "    for split, selected in subsets.items():\n",
    "        split_dir = os.path.join(\"dataset\", split, char_label)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            list(tqdm(\n",
    "                executor.map(\n",
    "                    lambda img: copy_image(os.path.join(char_folder, img),\n",
    "                                           os.path.join(split_dir, img)),\n",
    "                    selected\n",
    "                ),\n",
    "                total=len(selected),\n",
    "                desc=f\"{char_label}-{split}\",\n",
    "                leave=False\n",
    "            ))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chinese_CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Chinese_CNN_Model, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbafc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_classes = []\n",
    "for c in os.listdir(\"dataset/train\"):\n",
    "    if not os.listdir(os.path.join(\"dataset/train\", c)):\n",
    "        empty_classes.append(c)\n",
    "\n",
    "print(\"Empty classes:\", empty_classes[:10])\n",
    "print(\"Total empty:\", len(empty_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85479822",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"dataset\"\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_dir = os.path.join(root_dir, split)\n",
    "    for char_label in os.listdir(split_dir):\n",
    "        char_path = os.path.join(split_dir, char_label)\n",
    "        if not os.listdir(char_path):  \n",
    "            os.rmdir(char_path)\n",
    "            print(f\"Deleted empty folder: {char_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c05709",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "min_images = 100\n",
    "\n",
    "train_path = \"dataset/train\"\n",
    "value_path   = \"dataset/value\"\n",
    "test_path  = \"dataset/test\"\n",
    "\n",
    "for root in [train_path, value_path, test_path]:\n",
    "    for cls in os.listdir(root):\n",
    "        cls_path = os.path.join(root, cls)\n",
    "        if os.path.isdir(cls_path) and len(os.listdir(cls_path)) < min_images:\n",
    "            shutil.rmtree(cls_path) \n",
    "\n",
    "train_dataset_identification = datasets.ImageFolder(root=\"dataset/train\", transform=transform)\n",
    "value_dataset_identification  = datasets.ImageFolder(root=\"dataset/value\", transform=transform)\n",
    "test_dataset_identification = datasets.ImageFolder(root=\"dataset/test\", transform=transform)\n",
    "\n",
    "train_loader_identification = DataLoader(train_dataset_identification, batch_size=64, shuffle=True)\n",
    "value_loader_identification   = DataLoader(value_dataset_identification, batch_size=64, shuffle=False)\n",
    "test_loader_identification  = DataLoader(test_dataset_identification, batch_size=64, shuffle=False)\n",
    "num_classes = len(train_dataset_identification.classes)\n",
    "\n",
    "train_classes = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path,d))]\n",
    "value_classes   = [d for d in os.listdir(value_path)   if os.path.isdir(os.path.join(value_path,d))]\n",
    "test_classes  = [d for d in os.listdir(test_path)  if os.path.isdir(os.path.join(test_path,d))]\n",
    "\n",
    "print(f\"Train classes: {len(train_classes)}\")\n",
    "print(f\"Val classes: {len(value_classes)}\")\n",
    "print(f\"Test classes: {len(test_classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "393c1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classes: ['一', '丁', '七', '万', '三', '上', '下', '不', '与', '专', '世', '业', '东', '丝', '两', '严', '个', '中', '丰', '串', '临', '为', '主', '丽', '久', '义', '之', '乐', '乘', '九', '习', '乡', '书', '买', '了', '争', '事', '二', '于', '云', '五', '井', '亚', '交', '产', '享', '京', '亭', '亮', '人']\n"
     ]
    }
   ],
   "source": [
    "selected_classes = train_dataset_identification.classes[:50]\n",
    "print(\"Selected classes:\", selected_classes)\n",
    "\n",
    "selected_indices = [i for i, (_, label) in enumerate(train_dataset_identification.samples) if label < 50]\n",
    "subset_dataset = Subset(train_dataset_identification, selected_indices)\n",
    "\n",
    "total_len = len(subset_dataset)\n",
    "train_size = int(0.8 * total_len)\n",
    "val_size = int(0.1 * total_len)\n",
    "test_size = total_len - train_size - val_size\n",
    "\n",
    "train_dataset_selected, val_dataset_selected, test_dataset_selected = random_split(\n",
    "    subset_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader_selected = DataLoader(train_dataset_selected, batch_size=32, shuffle=True)\n",
    "val_loader_selected = DataLoader(val_dataset_selected, batch_size=32, shuffle=False)\n",
    "test_loader_selected = DataLoader(test_dataset_selected, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes_selected = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Chinese_CNN_Model(num_classes=num_classes_selected).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f753021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Loss: 2.7966 | Accuracy: 31.07%\n",
      "Epoch [2/25] | Loss: 1.3653 | Accuracy: 64.69%\n",
      "Epoch [3/25] | Loss: 1.0190 | Accuracy: 73.26%\n",
      "Epoch [4/25] | Loss: 0.8785 | Accuracy: 76.71%\n",
      "Epoch [5/25] | Loss: 0.7805 | Accuracy: 78.93%\n",
      "Epoch [6/25] | Loss: 0.7090 | Accuracy: 80.62%\n",
      "Epoch [7/25] | Loss: 0.6482 | Accuracy: 82.20%\n",
      "Epoch [8/25] | Loss: 0.6021 | Accuracy: 83.03%\n",
      "Epoch [9/25] | Loss: 0.5651 | Accuracy: 83.98%\n",
      "Epoch [10/25] | Loss: 0.5159 | Accuracy: 85.27%\n",
      "Epoch [11/25] | Loss: 0.4792 | Accuracy: 86.12%\n",
      "Epoch [12/25] | Loss: 0.4565 | Accuracy: 86.73%\n",
      "Epoch [13/25] | Loss: 0.4227 | Accuracy: 87.59%\n",
      "Epoch [14/25] | Loss: 0.4001 | Accuracy: 88.05%\n",
      "Epoch [15/25] | Loss: 0.3819 | Accuracy: 88.56%\n",
      "Epoch [16/25] | Loss: 0.3526 | Accuracy: 89.38%\n",
      "Epoch [17/25] | Loss: 0.3426 | Accuracy: 89.59%\n",
      "Epoch [18/25] | Loss: 0.3257 | Accuracy: 89.91%\n",
      "Epoch [19/25] | Loss: 0.3121 | Accuracy: 90.47%\n",
      "Epoch [20/25] | Loss: 0.2982 | Accuracy: 90.89%\n",
      "Epoch [21/25] | Loss: 0.2768 | Accuracy: 91.56%\n",
      "Epoch [22/25] | Loss: 0.2750 | Accuracy: 91.50%\n",
      "Epoch [23/25] | Loss: 0.2673 | Accuracy: 91.78%\n",
      "Epoch [24/25] | Loss: 0.2580 | Accuracy: 91.94%\n",
      "Epoch [25/25] | Loss: 0.2498 | Accuracy: 92.23%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_for_identification_slected = Chinese_CNN_Model(num_classes=num_classes_selected).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_for_identification_slected.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model_for_identification_slected.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_selected:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_for_identification_slected(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader_selected)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {average_loss:.2f} | Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad79e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chinese_cnn_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, precision, recall, f1 = evaluate_chinese_cnn_model(model_for_identification_slected, test_loader_selected, device)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1-score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52457a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.001 classes = 50\n",
    "\"\"\"\n",
    "Accuracy: 85.24%\n",
    "Precision: 87.15%\n",
    "Recall: 74.14%\n",
    "F1-score: 78.97%\n",
    "\"\"\"\n",
    "#lr = 0.001 classes = 200\n",
    "\"\"\"\n",
    "Accuracy: 71.78%\n",
    "Precision: 78.43%\n",
    "Recall: 57.07%\n",
    "F1-score: 63.69%\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Accuracy: 74.84%\n",
    "Precision: 78.59%\n",
    "Recall: 62.63%\n",
    "F1-score: 68.20%\n",
    "\"\"\"\n",
    "#lr = 0.001 classes = 500\n",
    "\"\"\"\n",
    "Accuracy: 66.12%\n",
    "Precision: 77.25%\n",
    "Recall: 51.29%\n",
    "F1-score: 59.06%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_for_identification_slected.state_dict(), \"Chinese_CNN_Model_500.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
